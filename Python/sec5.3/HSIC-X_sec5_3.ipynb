{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2d847-91be-4f37-8247-adbfff1e4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#sys.path.append(\"../\")\n",
    "from helpers.trainer import train_HSIC_IV\n",
    "from models.hsicx import LinearHSICX\n",
    "from models.kernel import RBFKernel, CmeangoryKernel\n",
    "from helpers.utils import to_torch, med_sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4805fb-bc82-4ca1-8fec-4e58829489cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'batch_size': 256, 'lr': 1e-2,\n",
    "               'max_epoch': 700, 'num_restart': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd315e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for datasets\n",
    "datasets = {\n",
    "    \"1000\": {\n",
    "        \"train\": '../../../data_sim/results/train_Zbin_g_mult_g_log_f_lin1000.csv',\n",
    "        \"test\": '../../../data_sim/results/test_Zbin_g_mult_g_log_f_lin1000.csv',\n",
    "    },\n",
    "    \"10000\": {\n",
    "        \"train\": '../../../data_sim/results/train_Zbin_g_mult_g_log_f_lin10000.csv',\n",
    "        \"test\": '../../../data_sim/results/test_Zbin_g_mult_g_log_f_lin10000.csv',\n",
    "    }\n",
    "}\n",
    "\n",
    "instrument = 'Binary'  # 'Binary' or 'Continuous'\n",
    "\n",
    "def run_single_rep(i, X, Y, Z, X_test, X_test_grid, config):\n",
    "    # HSIC IV model setup\n",
    "    s_z = med_sigma(Z)\n",
    "    kernel_e = RBFKernel(sigma=1)\n",
    "    kernel_z = CategoryKernel() if instrument == 'Binary' else RBFKernel(sigma=s_z)\n",
    "\n",
    "    # Train HSIC IV model\n",
    "    hsic_net = LinearHSICX(input_dim=2, lr=config['lr'], lmd=0.0, kernel_e=kernel_e, kernel_z=kernel_z, bias=False)\n",
    "    hsic_net = train_HSIC_IV(hsic_net, config, X, Y, Z, verbose=True)\n",
    "\n",
    "    intercept_adjust = Y.mean() - hsic_net(to_torch(X)).mean()\n",
    "    y_hat_hsic = intercept_adjust + hsic_net(to_torch(X_test))\n",
    "    y_hat_hsic_grid = intercept_adjust + hsic_net(to_torch(X_test_grid))\n",
    "\n",
    "    return (\n",
    "        y_hat_hsic.detach().numpy().copy(),\n",
    "        y_hat_hsic_grid.detach().numpy().copy(),\n",
    "        hsic_net.layers[0].weight.data.flatten().tolist()\n",
    "    )\n",
    "\n",
    "for size, paths in datasets.items():\n",
    "    # Load train and test data\n",
    "    train_data = np.genfromtxt(paths['train'], delimiter=',', skip_header=1)\n",
    "    test_data = np.genfromtxt(paths['test'], delimiter=',', skip_header=1)\n",
    "\n",
    "    # Prepare data\n",
    "    Z, X, Y = train_data[:, 0], train_data[:, 1:3], train_data[:, 3]\n",
    "    X_test = test_data[:, 0:2].astype(np.float32)\n",
    "    X_test_grid = test_data[:, 3:5].astype(np.float32)\n",
    "\n",
    "    # DataFrames to store results\n",
    "    df_mse = pd.DataFrame()\n",
    "    df_beta = pd.DataFrame()\n",
    "\n",
    "    # Train MSE model to get initial coefficients\n",
    "    # mse_reg = PredPolyRidge(degree=1, bias=False)\n",
    "    # mse_reg.fit(X, Y)\n",
    "    # mse_coef = mse_reg.reg.coef_\n",
    "\n",
    "    # Parallelize 10 repetitions\n",
    "    results = Parallel(n_jobs=10)(delayed(run_single_rep)(\n",
    "        i, X, Y, Z, X_test, X_test_grid, config\n",
    "    ) for i in range(10))\n",
    "\n",
    "    # Combine results into DataFrames\n",
    "    df_mse = pd.DataFrame({f'Run_{i+1}': result[0] for i, result in enumerate(results)})\n",
    "    df_beta = pd.DataFrame({f'Run_{i+1}': result[2] for i, result in enumerate(results)})\n",
    "\n",
    "    # Save results for this dataset size\n",
    "    # df_mse.to_csv(f'../output_data/hsic_lin_result_mse_UI_{size}.csv', index=False)\n",
    "    df_beta.to_csv(f'../output_data/hsic_lin_result_beta_UI_{size}.csv', index=False)\n",
    "\n",
    "    print(f\"Finished processing dataset size {size}.\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m87"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
