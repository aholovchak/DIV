{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from joblib import Parallel, delayed\n",
    "from methods.toy_model_selection_method import ToyModelSelectionMethod # change input dimension! (f model(s) for X, g model for Y)\n",
    "\n",
    "\n",
    "# Load dataset in the same way as HSIC-X\n",
    "dataset_rpe1 = pd.read_csv(\"data/sec6.2/dataset_rpe1.csv\")\n",
    "\n",
    "# Select relevant columns (same as HSIC-X)\n",
    "interv_genes = dataset_rpe1.columns[:9].tolist()\n",
    "train_data = dataset_rpe1[dataset_rpe1['interventions'].isin(interv_genes + [\"non-targeting\"])].copy()\n",
    "\n",
    "# Convert intervention column to categorical\n",
    "train_data['interventions'] = train_data['interventions'].astype('category')\n",
    "train_data['Ztr'] = train_data.iloc[:, 10].astype('category')\n",
    "\n",
    "# Get list of unique interventions (excluding \"non-targeting\")\n",
    "unique_interventions = [g for g in train_data['interventions'].unique() if g != \"non-targeting\"]\n",
    "\n",
    "# Define test data (from 50 test environments)\n",
    "test_data_path = 'data/sec6.2/test_single_cell.csv'\n",
    "test_data = torch.tensor(np.genfromtxt(test_data_path, delimiter=',', skip_header=1), dtype=torch.float32)\n",
    "Xtest = test_data[:, 0:9].reshape(-1, 9)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to process a single intervention removal\n",
    "def gmm_train_stability(i, gene_to_remove, train_data, interv_genes):\n",
    "    print(f\"Iteration {i+1}: Removing intervention {gene_to_remove}\")\n",
    "\n",
    "    # Remove data for the given intervention\n",
    "    valid_rows = train_data['interventions'].isin([\"non-targeting\"] + list(set(interv_genes) - {gene_to_remove}))\n",
    "\n",
    "    # Apply filtering\n",
    "    filtered_data = train_data[valid_rows]\n",
    "\n",
    "    # Convert `Ztr` to One-Hot Encoding AFTER removing one environment\n",
    "    encoder = OneHotEncoder(sparse_output=False)  # Drop first category for consistency\n",
    "    Ztr_encoded = encoder.fit_transform(filtered_data[['Ztr']])\n",
    "\n",
    "    # Extract features and target\n",
    "    Xtr = filtered_data.iloc[:, :9].values  # First 9 columns\n",
    "    Ytr = filtered_data.iloc[:, 9].values   # 10th column\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(Xtr, dtype=torch.float32)\n",
    "    Y_train = torch.tensor(Ytr.reshape(-1, 1), dtype=torch.float32)\n",
    "    Z_train = torch.tensor(Ztr_encoded, dtype=torch.float32)\n",
    "\n",
    "    # Define PyTorch Dataset\n",
    "    class MyDataset(Dataset):\n",
    "        def __init__(self, X, Z, Y):\n",
    "            self.X = X\n",
    "            self.Z = Z\n",
    "            self.Y = Y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.Z[idx], self.Y[idx]\n",
    "\n",
    "    # Create dataset and split into training/validation sets\n",
    "    dataset = MyDataset(X_train, Z_train, Y_train)\n",
    "    train_ratio = 0.9\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    train_data, val_data = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Extract separate tensors for training and validation\n",
    "    X_train, Z_train, Y_train = train_data.dataset.X[:train_size], train_data.dataset.Z[:train_size], train_data.dataset.Y[:train_size]\n",
    "    X_val, Z_val, Y_val = val_data.dataset.X[train_size:], val_data.dataset.Z[train_size:], val_data.dataset.Y[train_size:]\n",
    "\n",
    "    # Train DeepGMM model\n",
    "    \n",
    "    deepGMM = ToyModelSelectionMethod()\n",
    "    deepGMM.fit(X_train.double(), Z_train.double(), Y_train.double(),\n",
    "                X_val.double(), Z_val.double(), Y_val.double(),\n",
    "                g_dev=None, verbose=True)\n",
    "\n",
    "    # Make predictions on the same test data as HSIC-X\n",
    "    y_hat_deepGMM = deepGMM.predict(torch.tensor(Xtest, dtype=torch.float32).double())\n",
    "    y_hat_deepGMM = y_hat_deepGMM.detach().numpy().copy()\n",
    "\n",
    "    return i, y_hat_deepGMM  # Return index and predictions\n",
    "\n",
    "\n",
    "# Parallel execution for all interventions\n",
    "if __name__ == \"__main__\":\n",
    "    num_iterations = len(unique_interventions)\n",
    "\n",
    "    results = Parallel(n_jobs=9)(\n",
    "        delayed(gmm_train_stability)(i, gene_to_remove, train_data, interv_genes)\n",
    "        for i, gene_to_remove in enumerate(unique_interventions)\n",
    "    )\n",
    "\n",
    "    # Store predictions in DataFrame\n",
    "    df_results = pd.DataFrame({f'Run_{i+1}': y_hat_deepGMM.flatten() for i, y_hat_deepGMM in results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.columns = unique_interventions\n",
    "\n",
    "df_results.to_csv(\"results/sec6.2/deepgmm_singlecell_pairwise.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
