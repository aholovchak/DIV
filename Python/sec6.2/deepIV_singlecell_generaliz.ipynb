{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.iv.nnet import DeepIV\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset in the same way as HSIC-X\n",
    "dataset_rpe1 = pd.read_csv(\"~/Documents/ethz/DIV/single_cell/dataset_rpe1.csv\")\n",
    "\n",
    "# Select relevant columns (same as HSIC-X)\n",
    "interv_genes = dataset_rpe1.columns[:9].tolist()\n",
    "train_data = dataset_rpe1[dataset_rpe1['interventions'].isin(interv_genes + [\"non-targeting\"])].copy()\n",
    "\n",
    "# Convert intervention column to categorical\n",
    "train_data['interventions'] = train_data['interventions'].astype('category')\n",
    "train_data['Ztr'] = train_data.iloc[:, 10].astype('category')\n",
    "\n",
    "# Get list of unique interventions (excluding \"non-targeting\")\n",
    "unique_interventions = [g for g in train_data['interventions'].unique() if g != \"non-targeting\"]\n",
    "\n",
    "# Convert `Ztr` to One-Hot Encoding AFTER removing one environment\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Drop first category for consistency\n",
    "Ztr_encoded = encoder.fit_transform(train_data[['Ztr']])\n",
    "\n",
    "# Extract features and target\n",
    "Xtr = train_data.iloc[:, :9].values  # First 9 columns (gene expressions)\n",
    "Ytr = train_data.iloc[:, 9].values   # 10th column (response variable)\n",
    "\n",
    "# Convert to NumPy arrays for compatibility with the original method\n",
    "data_train_length = Xtr.shape[0]\n",
    "\n",
    "# Reformat to match the original method's expected structure\n",
    "z = Ztr_encoded  # Encoded intervention variable\n",
    "t = Xtr          # First 9 columns (gene expressions)\n",
    "y = Ytr          # 10th column (response variable)\n",
    "x = np.zeros(data_train_length)  # Dummy placeholder if needed for consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape, t.shape, y.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'z (Instrument): {z}')\n",
    "print(f't (Treatment): {t}')\n",
    "print(f'y (Outcome): {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_model = keras.Sequential([keras.layers.Dense(256, activation='relu', input_shape=(11,)),\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(128, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(64, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(32, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17)])\n",
    "\n",
    "response_model = keras.Sequential([keras.layers.Dense(256, activation='relu', input_shape=(10,)),\n",
    "                                   keras.layers.Dropout(0.17),\n",
    "                                   keras.layers.Dense(128, activation='relu'),\n",
    "                                   keras.layers.Dropout(0.17),\n",
    "                                   keras.layers.Dense(64, activation='relu'),\n",
    "                                   keras.layers.Dropout(0.17),\n",
    "                                   keras.layers.Dense(32, activation='relu'),\n",
    "                                   keras.layers.Dropout(0.17),\n",
    "                                   keras.layers.Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = 'R/sec6.2/test_single_cell.csv'\n",
    "test_data = np.genfromtxt(test_data_path, delimiter=',', skip_header=1)\n",
    "X_test = test_data[:, 0:9].reshape(-1, 9)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and predict DeepIV for a single run\n",
    "def train_and_predict(run_id):\n",
    "    print(f\"Starting DeepIV Run {run_id + 1}/10...\")\n",
    "\n",
    "    keras_fit_options = { \"epochs\": 30,\n",
    "                      \"validation_split\": 0.2,\n",
    "                      \"callbacks\": [keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)]}\n",
    "\n",
    "    deepIvEst = DeepIV(n_components = 10, # number of gaussians in our mixture density network\n",
    "                m = lambda z, x : treatment_model(keras.layers.concatenate([z,x])), # treatment model\n",
    "                h = lambda t, x : response_model(keras.layers.concatenate([t,x])),  # response model\n",
    "                n_samples = 1, # number of samples to use to estimate the response\n",
    "                use_upper_bound_loss = False, # whether to use an approximation to the true loss\n",
    "                n_gradient_samples = 1, # number of samples to use in second estimate of the response (to make loss estimate unbiased)\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=0.0005), # Keras optimizer to use for training - see https://keras.io/optimizers/ \n",
    "                first_stage_options = keras_fit_options, # options for training treatment model\n",
    "                second_stage_options = keras_fit_options) # options for training response model\n",
    "\n",
    "    # Train DeepIV model\n",
    "    deepIvEst.fit(Y=y, T=t, X=x, Z=z)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_hat_deepIV = deepIvEst.predict(X_test, np.zeros(X_test.shape[0]))\n",
    "\n",
    "    return y_hat_deepIV\n",
    "\n",
    "# Run the 10 iterations in parallel\n",
    "num_repeats = 10\n",
    "results = Parallel(n_jobs=num_repeats)(\n",
    "    delayed(train_and_predict)(i) for i in range(num_repeats)\n",
    ")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results).T  # Transpose to have runs as columns\n",
    "\n",
    "# Rename columns to indicate run numbers\n",
    "df_results.columns = [f'Run_{i+1}' for i in range(num_repeats)]\n",
    "\n",
    "# Save the results as a CSV file\n",
    "output_filename = 'results/deepIV_singlecell_10runs.csv'\n",
    "df_results.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"DeepIV predictions saved to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepIV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
