{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import torch\n",
    "from econml.iv.nnet import DeepIV\n",
    "\n",
    "# Paths to the 10 training and testing datasets\n",
    "train_files = [\n",
    "    'data/sec5.1/train_Zbin_g_lin_f_lin.csv',\n",
    "    'data/sec5.1/train_Zcont_g_lin_f_lin.csv',\n",
    "    'data/sec5.1/train_Zbin_g_lin_f_log_case.csv',\n",
    "    'data/sec5.1/train_Zcont_g_lin_f_log_case.csv',\n",
    "    'data/sec5.1/train_Zbin_g_lin_f_sin_lin.csv',\n",
    "    'data/sec5.1/train_Zcont_g_lin_f_sin_lin.csv'\n",
    "]\n",
    "\n",
    "test_files = [\n",
    "    'data/sec5.1/test_Zbin_g_lin_f_lin.csv',\n",
    "    'data/sec5.1/test_Zcont_g_lin_f_lin.csv',\n",
    "    'data/sec5.1/test_Zbin_g_lin_f_log_case.csv',\n",
    "    'data/sec5.1/test_Zcont_g_lin_f_log_case.csv',\n",
    "    'data/sec5.1/test_Zbin_g_lin_f_sin_lin.csv',\n",
    "    'data/sec5.1/test_Zcont_g_lin_f_sin_lin.csv'\n",
    "]\n",
    "\n",
    "# Define the treatment and response models\n",
    "treatment_model = keras.Sequential([keras.layers.Dense(256, activation='relu', input_shape=(2,)),\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(128, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(64, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(32, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17)])\n",
    "\n",
    "response_model = keras.Sequential([keras.layers.Dense(256, activation='relu', input_shape=(2,)),\n",
    "                                   keras.layers.Dropout(0.17),\n",
    "                                   keras.layers.Dense(128, activation='relu'),\n",
    "                                   keras.layers.Dropout(0.17),\n",
    "                                   keras.layers.Dense(64, activation='relu'),\n",
    "                                   keras.layers.Dropout(0.17),\n",
    "                                   keras.layers.Dense(32, activation='relu'),\n",
    "                                   keras.layers.Dropout(0.17),\n",
    "                                   keras.layers.Dense(1)])\n",
    "\n",
    "# Options for Keras model training\n",
    "keras_fit_options = { \"epochs\": 30,\n",
    "                      \"validation_split\": 0.2,\n",
    "                      \"callbacks\": [keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)]}\n",
    "\n",
    "# Loop over each dataset\n",
    "for idx, (train_file, test_file) in enumerate(zip(train_files, test_files), start=1):\n",
    "    \n",
    "    # Load train and test data\n",
    "    train_data = np.genfromtxt(train_file, delimiter=',', skip_header=1)\n",
    "    test_data = np.genfromtxt(test_file, delimiter=',', skip_header=1)\n",
    "\n",
    "    data_length = train_data.shape[0]\n",
    "    print(f'Train data size for DGP{idx}: {data_length}')\n",
    "\n",
    "    # Extract variables\n",
    "    z = train_data[:, 1]\n",
    "    t = train_data[:, 2]\n",
    "    y = train_data[:, 3]\n",
    "    x = np.zeros(data_length)  # No exogenous variables\n",
    "    \n",
    "    # Initialize the DeepIV model\n",
    "    deepIvEst = DeepIV(n_components = 10,\n",
    "                       m = lambda z, x : treatment_model(keras.layers.concatenate([z,x])), \n",
    "                       h = lambda t, x : response_model(keras.layers.concatenate([t,x])),\n",
    "                       n_samples = 1,\n",
    "                       use_upper_bound_loss = False,\n",
    "                       n_gradient_samples = 1,\n",
    "                       optimizer='adam',\n",
    "                       first_stage_options = keras_fit_options,\n",
    "                       second_stage_options = keras_fit_options)\n",
    "\n",
    "    # Prepare test data as tensors\n",
    "    X_test = torch.tensor(test_data[:, 0].astype(np.float32)).squeeze()\n",
    "    X_test_grid = torch.tensor(test_data[:, 4].astype(np.float32)).squeeze()\n",
    "\n",
    "    # Initialize DataFrames for storing results\n",
    "    df_mse = pd.DataFrame()\n",
    "    df_plot = pd.DataFrame()\n",
    "\n",
    "    # Run training and predictions 10 times for each dataset\n",
    "    for run in range(10):\n",
    "        # Fit the DeepIV model\n",
    "        deepIvEst.fit(Y=y, T=t, X=x, Z=z)\n",
    "\n",
    "        # Generate predictions for each x_temp\n",
    "        y_hat_deepiv = deepIvEst.predict(X_test, np.full_like(X_test, x[0]))\n",
    "        y_hat_deepiv_grid = deepIvEst.predict(X_test_grid, np.full_like(X_test_grid, x[0]))\n",
    "\n",
    "        # Add the predictions as a new column to the DataFrame\n",
    "        df_mse[f'Run_{run+1}'] = y_hat_deepiv\n",
    "        df_plot[f'Run_{run+1}'] = y_hat_deepiv_grid\n",
    "\n",
    "    # Save the DataFrames to CSV files for each dataset\n",
    "    df_mse.to_csv(f'results/sec5.1/deepiv_result_mse_dgp{idx}.csv', index=False)\n",
    "    df_plot.to_csv(f'results/sec5.1/deepiv_result_plot_dgp{idx}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepIV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
